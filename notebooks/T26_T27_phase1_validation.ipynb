{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1 Validation Harness (T26/T27)\n",
        "This notebook exercises Phase 1 validations for deterministic SIM (T26) and paper safety rails (T27)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import json\nimport os\nimport sys\nfrom pathlib import Path\n\nfrom research.utils import (\n    discover_run_artifacts,\n    hash_json_normalized,\n    hash_jsonl_normalized,\n    run_cmd,\n)\n\nPROJECT_ROOT = Path('.').resolve()\nDATA_ROOT = Path(os.environ.get('QUANTO_DATA_ROOT', PROJECT_ROOT / '.quanto_data')).resolve()\nPROMOTION_DIR = DATA_ROOT / 'promotions' / 'candidate'\n\n\ndef _load_candidate_id() -> str:\n    override = os.environ.get('NOTEBOOK_CANDIDATE_ID')\n    if override:\n        return override.strip()\n    promos = sorted(PROMOTION_DIR.glob('*.json'))\n    if not promos:\n        raise RuntimeError(f'No candidate promotions found in {PROMOTION_DIR}')\n    payload = json.loads(promos[0].read_text(encoding='utf-8'))\n    experiment_id = payload.get('experiment_id')\n    if not experiment_id:\n        raise RuntimeError(f'Promotion record {promos[0]} missing experiment_id')\n    return experiment_id\n\n\nCANDIDATE_ID = _load_candidate_id()\nBASELINE_ID = CANDIDATE_ID\nSPEC_PATH = DATA_ROOT / 'experiments' / CANDIDATE_ID / 'spec' / 'experiment_spec.json'\nSPEC_PAYLOAD = json.loads(SPEC_PATH.read_text(encoding='utf-8'))\nSTART_DATE = SPEC_PAYLOAD.get('start_date')\nEND_DATE = SPEC_PAYLOAD.get('end_date')\nSYMBOLS = SPEC_PAYLOAD.get('symbols') or []\nprint(f'Candidate: {CANDIDATE_ID}')\nprint(f'Window: {START_DATE} -> {END_DATE}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import shutil\n\n\ndef run_shadow_once(output_dir: Path, *, max_steps: int | None = None, resume: bool = False, reset: bool = False):\n    output_dir = Path(output_dir)\n    output_dir.parent.mkdir(parents=True, exist_ok=True)\n    cmd = [\n        sys.executable,\n        'scripts/run_shadow.py',\n        '--experiment-id', CANDIDATE_ID,\n        '--replay',\n        '--start-date', START_DATE,\n        '--end-date', END_DATE,\n        '--output-dir', str(output_dir),\n    ]\n    if max_steps is not None:\n        cmd += ['--max-steps', str(max_steps)]\n    if resume:\n        cmd.append('--resume')\n    if reset:\n        cmd.append('--reset')\n    result = run_cmd(cmd, cwd=PROJECT_ROOT, check=False)\n    if result.stdout:\n        print(result.stdout.strip())\n    if result.stderr:\n        print(result.stderr.strip())\n    return result\n\n\ndef gather_hashes(run_dir: Path) -> dict[str, str]:\n    artifacts = discover_run_artifacts(Path(run_dir))\n    hashes: dict[str, str] = {}\n    hashes['steps'] = hash_jsonl_normalized(artifacts['steps'])\n    if 'metrics' in artifacts:\n        hashes['metrics'] = hash_json_normalized(artifacts['metrics'])\n    if 'execution_metrics' in artifacts:\n        hashes['execution_metrics'] = hash_json_normalized(artifacts['execution_metrics'])\n    print(f\"Hashes for {run_dir} -> {hashes}\")\n    return hashes\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "for script in ('scripts/run_shadow.py', 'scripts/run_paper.py', 'scripts/qualify_experiment.py'):\n    print(f'=== {script} --help ===')\n    result = run_cmd([sys.executable, script, '-h'], cwd=PROJECT_ROOT, check=False)\n    print(result.stdout)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sim_root = DATA_ROOT / 'shadow' / CANDIDATE_ID\nrun_one = sim_root / 'notebook_sim_run1'\nrun_two = sim_root / 'notebook_sim_run2'\nrun_shadow_once(run_one, reset=True)\nrun_shadow_once(run_two, reset=True)\nhashes_one = gather_hashes(run_one)\nhashes_two = gather_hashes(run_two)\nassert hashes_one['steps'] == hashes_two['steps'], 'Twin-run determinism violated'\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "resume_run = sim_root / 'notebook_sim_resume'\nreference_run = sim_root / 'notebook_sim_reference_full'\npartial_steps = 2\nrun_shadow_once(resume_run, max_steps=partial_steps, reset=True)\nrun_shadow_once(resume_run, resume=True)\nrun_shadow_once(reference_run, reset=True)\nhash_resume = gather_hashes(resume_run)\nhash_reference = gather_hashes(reference_run)\nassert hash_resume['steps'] == hash_reference['steps'], 'Resume hashes mismatch'\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paper_root = DATA_ROOT / 'paper' / CANDIDATE_ID\n",
        "paper_root.mkdir(parents=True, exist_ok=True)\n",
        "failfast_config = paper_root / 'notebook_failfast.json'\n",
        "universe = SYMBOLS[:2] if len(SYMBOLS) >= 2 else ['AAPL', 'MSFT']\n",
        "base_payload = {\n",
        "    'experiment_id': CANDIDATE_ID,\n",
        "    'execution_mode': 'alpaca_paper',\n",
        "    'universe': universe,\n",
        "    'broker': {'alpaca_base_url': 'https://paper-api.alpaca.markets'},\n",
        "}\n",
        "failfast_config.write_text(json.dumps(base_payload, indent=2), encoding='utf-8')\n",
        "env_failfast = dict(os.environ)\n",
        "env_failfast.pop('ALPACA_API_KEY', None)\n",
        "env_failfast.pop('ALPACA_SECRET_KEY', None)\n",
        "result_failfast = run_cmd([sys.executable, 'scripts/run_paper.py', '--config', str(failfast_config)], cwd=PROJECT_ROOT, env=env_failfast, check=False)\n",
        "print(result_failfast.stdout)\n",
        "print(result_failfast.stderr)\n",
        "assert result_failfast.returncode != 0, 'Expected fail-fast without credentials'\n",
        "\n",
        "live_payload = dict(base_payload)\n",
        "live_payload['broker'] = {'alpaca_base_url': 'https://api.alpaca.markets'}\n",
        "live_config = paper_root / 'notebook_live_url.json'\n",
        "live_config.write_text(json.dumps(live_payload, indent=2), encoding='utf-8')\n",
        "env_live = dict(os.environ)\n",
        "env_live['ALPACA_API_KEY'] = env_live.get('ALPACA_API_KEY', 'demo')\n",
        "env_live['ALPACA_SECRET_KEY'] = env_live.get('ALPACA_SECRET_KEY', 'demo')\n",
        "result_live = run_cmd([sys.executable, 'scripts/run_paper.py', '--config', str(live_config)], cwd=PROJECT_ROOT, env=env_live, check=False)\n",
        "print(result_live.stdout)\n",
        "print(result_live.stderr)\n",
        "assert result_live.returncode != 0, 'Expected live URL rejection'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "registry_root = DATA_ROOT / 'experiments'\nqual_cmd = [\n    sys.executable,\n    'scripts/qualify_experiment.py',\n    '--experiment-id', CANDIDATE_ID,\n    '--baseline', BASELINE_ID,\n    '--registry-root', str(registry_root),\n]\nqual_result = run_cmd(qual_cmd, cwd=PROJECT_ROOT, check=False)\nprint(qual_result.stdout)\nprint(qual_result.stderr)\nassert qual_result.returncode == 0\nreport_path = registry_root / CANDIDATE_ID / 'promotion' / 'qualification_report.json'\nprint(f'Qualification report: {report_path}')\ncomp_cmd = [\n    sys.executable,\n    'scripts/compare_experiments.py',\n    '--candidate', CANDIDATE_ID,\n    '--baseline', BASELINE_ID,\n    '--registry-root', str(registry_root),\n]\ncomp_result = run_cmd(comp_cmd, cwd=PROJECT_ROOT, check=False)\nprint(comp_result.stdout)\nprint(comp_result.stderr)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}