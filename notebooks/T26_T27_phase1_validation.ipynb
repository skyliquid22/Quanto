{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40411a2",
   "metadata": {},
   "source": [
    "# Phase 1 Validation Harness (T26/T27)\n",
    "This notebook exercises Phase 1 validations for deterministic SIM (T26) and paper safety rails (T27)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba35649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from research.utils import (\n",
    "    discover_run_artifacts,\n",
    "    hash_json_normalized,\n",
    "    hash_jsonl_normalized,\n",
    "    run_cmd,\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_ROOT = Path(os.environ.get('QUANTO_DATA_ROOT', PROJECT_ROOT / '.quanto_data')).resolve()\n",
    "PROMOTION_DIR = DATA_ROOT / 'promotions' / 'candidate'\n",
    "\n",
    "\n",
    "def _load_candidate_id() -> str:\n",
    "    override = os.environ.get('NOTEBOOK_CANDIDATE_ID')\n",
    "    if override:\n",
    "        return override.strip()\n",
    "    promos = sorted(PROMOTION_DIR.glob('*.json'))\n",
    "    if not promos:\n",
    "        raise RuntimeError(f'No candidate promotions found in {PROMOTION_DIR}')\n",
    "    payload = json.loads(promos[0].read_text(encoding='utf-8'))\n",
    "    experiment_id = payload.get('experiment_id')\n",
    "    if not experiment_id:\n",
    "        raise RuntimeError(f'Promotion record {promos[0]} missing experiment_id')\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "CANDIDATE_ID = _load_candidate_id()\n",
    "BASELINE_ID = CANDIDATE_ID\n",
    "SPEC_PATH = DATA_ROOT / 'experiments' / CANDIDATE_ID / 'spec' / 'experiment_spec.json'\n",
    "SPEC_PAYLOAD = json.loads(SPEC_PATH.read_text(encoding='utf-8'))\n",
    "START_DATE = SPEC_PAYLOAD.get('start_date')\n",
    "END_DATE = SPEC_PAYLOAD.get('end_date')\n",
    "SYMBOLS = SPEC_PAYLOAD.get('symbols') or []\n",
    "print(f'Candidate: {CANDIDATE_ID}')\n",
    "print(f'Window: {START_DATE} -> {END_DATE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab960185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def run_shadow_once(output_dir: Path, *, max_steps: int | None = None, resume: bool = False, reset: bool = False):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        'scripts/run_shadow.py',\n",
    "        '--experiment-id', CANDIDATE_ID,\n",
    "        '--replay',\n",
    "        '--start-date', START_DATE,\n",
    "        '--end-date', END_DATE,\n",
    "        '--output-dir', str(output_dir),\n",
    "    ]\n",
    "    if max_steps is not None:\n",
    "        cmd += ['--max-steps', str(max_steps)]\n",
    "    if resume:\n",
    "        cmd.append('--resume')\n",
    "    if reset:\n",
    "        cmd.append('--reset')\n",
    "    result = run_cmd(cmd, cwd=PROJECT_ROOT, check=False)\n",
    "    if result.stdout:\n",
    "        print(result.stdout.strip())\n",
    "    if result.stderr:\n",
    "        print(result.stderr.strip())\n",
    "    return result\n",
    "\n",
    "\n",
    "def gather_hashes(run_dir: Path) -> dict[str, str]:\n",
    "    artifacts = discover_run_artifacts(Path(run_dir))\n",
    "    hashes: dict[str, str] = {}\n",
    "    hashes['steps'] = hash_jsonl_normalized(artifacts['steps'])\n",
    "    if 'metrics' in artifacts:\n",
    "        hashes['metrics'] = hash_json_normalized(artifacts['metrics'])\n",
    "    if 'execution_metrics' in artifacts:\n",
    "        hashes['execution_metrics'] = hash_json_normalized(artifacts['execution_metrics'])\n",
    "    print(f\"Hashes for {run_dir} -> {hashes}\")\n",
    "    return hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in ('scripts/run_shadow.py', 'scripts/run_paper.py', 'scripts/qualify_experiment.py'):\n",
    "    print(f'=== {script} --help ===')\n",
    "    result = run_cmd([sys.executable, script, '-h'], cwd=PROJECT_ROOT, check=False)\n",
    "    print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc880a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_root = DATA_ROOT / 'shadow' / CANDIDATE_ID\n",
    "run_one = sim_root / 'notebook_sim_run1'\n",
    "run_two = sim_root / 'notebook_sim_run2'\n",
    "run_shadow_once(run_one, reset=True)\n",
    "run_shadow_once(run_two, reset=True)\n",
    "hashes_one = gather_hashes(run_one)\n",
    "hashes_two = gather_hashes(run_two)\n",
    "assert hashes_one['steps'] == hashes_two['steps'], 'Twin-run determinism violated'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_run = sim_root / 'notebook_sim_resume'\n",
    "reference_run = sim_root / 'notebook_sim_reference_full'\n",
    "partial_steps = 2\n",
    "run_shadow_once(resume_run, max_steps=partial_steps, reset=True)\n",
    "run_shadow_once(resume_run, resume=True)\n",
    "run_shadow_once(reference_run, reset=True)\n",
    "hash_resume = gather_hashes(resume_run)\n",
    "hash_reference = gather_hashes(reference_run)\n",
    "assert hash_resume['steps'] == hash_reference['steps'], 'Resume hashes mismatch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_root = DATA_ROOT / 'paper' / CANDIDATE_ID\n",
    "paper_root.mkdir(parents=True, exist_ok=True)\n",
    "failfast_config = paper_root / 'notebook_failfast.json'\n",
    "universe = SYMBOLS[:2] if len(SYMBOLS) >= 2 else ['AAPL', 'MSFT']\n",
    "base_payload = {\n",
    "    'experiment_id': CANDIDATE_ID,\n",
    "    'execution_mode': 'alpaca_paper',\n",
    "    'universe': universe,\n",
    "    'broker': {'alpaca_base_url': 'https://paper-api.alpaca.markets'},\n",
    "}\n",
    "failfast_config.write_text(json.dumps(base_payload, indent=2), encoding='utf-8')\n",
    "env_failfast = dict(os.environ)\n",
    "env_failfast.pop('ALPACA_API_KEY', None)\n",
    "env_failfast.pop('ALPACA_SECRET_KEY', None)\n",
    "result_failfast = run_cmd([sys.executable, 'scripts/run_paper.py', '--config', str(failfast_config)], cwd=PROJECT_ROOT, env=env_failfast, check=False)\n",
    "print(result_failfast.stdout)\n",
    "print(result_failfast.stderr)\n",
    "assert result_failfast.returncode != 0, 'Expected fail-fast without credentials'\n",
    "\n",
    "live_payload = dict(base_payload)\n",
    "live_payload['broker'] = {'alpaca_base_url': 'https://api.alpaca.markets'}\n",
    "live_config = paper_root / 'notebook_live_url.json'\n",
    "live_config.write_text(json.dumps(live_payload, indent=2), encoding='utf-8')\n",
    "env_live = dict(os.environ)\n",
    "env_live['ALPACA_API_KEY'] = env_live.get('ALPACA_API_KEY', 'demo')\n",
    "env_live['ALPACA_SECRET_KEY'] = env_live.get('ALPACA_SECRET_KEY', 'demo')\n",
    "result_live = run_cmd([sys.executable, 'scripts/run_paper.py', '--config', str(live_config)], cwd=PROJECT_ROOT, env=env_live, check=False)\n",
    "print(result_live.stdout)\n",
    "print(result_live.stderr)\n",
    "assert result_live.returncode != 0, 'Expected live URL rejection'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_root = DATA_ROOT / 'experiments'\n",
    "qual_cmd = [\n",
    "    sys.executable,\n",
    "    'scripts/qualify_experiment.py',\n",
    "    '--experiment-id', CANDIDATE_ID,\n",
    "    '--baseline', BASELINE_ID,\n",
    "    '--registry-root', str(registry_root),\n",
    "]\n",
    "qual_result = run_cmd(qual_cmd, cwd=PROJECT_ROOT, check=False)\n",
    "print(qual_result.stdout)\n",
    "print(qual_result.stderr)\n",
    "assert qual_result.returncode == 0\n",
    "report_path = registry_root / CANDIDATE_ID / 'promotion' / 'qualification_report.json'\n",
    "print(f'Qualification report: {report_path}')\n",
    "comp_cmd = [\n",
    "    sys.executable,\n",
    "    'scripts/compare_experiments.py',\n",
    "    '--candidate', CANDIDATE_ID,\n",
    "    '--baseline', BASELINE_ID,\n",
    "    '--registry-root', str(registry_root),\n",
    "]\n",
    "comp_result = run_cmd(comp_cmd, cwd=PROJECT_ROOT, check=False)\n",
    "print(comp_result.stdout)\n",
    "print(comp_result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
